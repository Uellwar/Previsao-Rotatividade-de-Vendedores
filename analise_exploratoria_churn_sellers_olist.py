# -*- coding: utf-8 -*-
"""analise_exploratoria_churn_sellers_olist

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BX4mp1j0DY5liKFNSfhx3lB6BN1gAV20

<html>
<h1>Projeto Prevenção de Rotatividade de Vendedores 
<h4>(Etapa atual: Análise Exploratória)
<br><br>
<h4>Autor: Uéliton de Brito Viana

<h4>Linkedin: <a href="https://www.linkedin.com/in/ueliton-viana/">linkedin.com/in/ueliton-viana/</a>

<h4>Github: <a href="https://github.com/Uellwar">github.com/Uellwar</a>

<h4>Kaggle: <a href="https://www.kaggle.com/uelitonviana/code">kaggle.com/uelitonviana</a>

</html>

<html><h1> Apresentação do projeto:

O projeto se trata de uma hipotética situação, onde a empresa (de E-commerce) Olist gostaria de melhorar seus negócios por meio de Data Science. Em primeiro lugar, essa empresa define qual problema está tentando resolver. No caso, ela identificou que é cinco vezes mais caro adquirir novos vendedores do que reter os atuais. Logo, o problema é aumentar a taxa de retenção de vendedores para diminuir custos. Uma possível solução de Data Science é construir um modelo de Machine Learning para prever o churn. Depois, esse modelo é avaliado por meio de métricas de negócios e testes estatísticos e, por fim, é implementado.

Abaixo, encontra-se uma análise exploratória de nível iniciante com o propósito de encontrar insights nos dados disponibilizados, responder algumas dúvidas de negócio com base na exploração (utilizando Python e SQL) e preparar uma base de dados contendo as principais características para a próxima etapa do projeto: Feature Engineering.

(*A engenharia de recursos (Feature Engineering) refere-se ao processo de usar o conhecimento do domínio para selecionar e transformar as variáveis ​​mais relevantes de dados brutos ao criar um modelo preditivo usando aprendizado de máquina ou modelagem estatística.
<a href="https://www.heavy.ai/technical-glossary/feature-engineering">Fonte</a>*)

Ao final deste notebook, está disponível um dashboard interativo contendo as principais informações encontradas.

</html>

### Resumo
1. [✓] RELATÓRIO DAS INFORMAÇÕES ENCONTRADAS NA ETAPA EXPLORATÓRIA NO ENTENDIMENTO DOS DADOS

<html>

A base de dados possui um total de 91.342 vendas distintas, trazendo um total de R$ 14.891.744,80 ao longo de 22 meses (Out2016 até Ago2018) e apresenta  3.025 vendedores e 88.572 clientes/compradores

Característica | Qtd Total
---------------|----------
Vendas         | 91.342
Vendedores     | 3.025
Clientes       | 88.572

No entanto, os dois primeiros meses Out2016 e Dez2016 possuem pouquíssimas vendas na base e justamente por esse motivo, algumas análises foram feitas a partir de 2017.

Meses de 2016| Qtd de Vendas
-------------|--------------
Outubro      | 283
Dezembro     | 1


Quanto aos dados duplicados, forão encontradas e removidas 10.492 observações. E sobre os dados ausentes, apenas as colunas review_comment_title e review_comment_message apresentam porcentagens significantes com 88.43% e 58.93% respectivamente.

Ranking | Variável                   | % Faltante 
--------|----------------------------|-------------
1       | Título da avaliação        | 88.43
2       | Comentário da avaliação    | 58.93
3       | Data de entrega do produto | 2.13

<br>

Dentre os pedidos, a classe *delivered* (entregue) é a classe majoritária com 98% das vendas, seguida de shipped (enviado) com 1%. As formas de pagamento mais frequêntes são Cartão de Crédito (75%) e Boleto (19%). Quando olhamos para os parcelamentos, vimos que as vendas não parceladas correspondem a metade (50%) de todas as vendas, e que, as vendas com até 4 parcelas representam um total de 79%. No entanto, quando observamos a mediana dos valores das vendas distintas (R$ 102.86), fica fácil entender os números acima.

<br>

Quanto aos produtos, mais especificamente suas categorias, Cama_Mesa_Banho é a mais vendida apresentando 9.38% de todas as vendas (porém conta com uma nota média relativamente baixa nas avaliações). As outras 72 categorias dividem os 90,62%.
A base conta com 31.233 produtos distintos.
No entanto, se analisarmos as categorias visando lucratividade, Beleza_Saude é a categoria com a maior soma (R$ 1.532.127,56)	e uma média alta de 4.24 nas avaliações (5 é o máximo).

Ranking | Categoria              | % de Vendas  | Média Aval
--------|------------------------|--------------|-----------
1       | cama_mesa_banho        | 9.38         |3.92
2       | beleza_saude           | 9.04         |4.19
3       | esporte_lazer          | 7.69         |4.19
4       | informatica_acessorios | 6.59         |4.05
5       | relogios_presentes     | 5.93         |4.06

<br>

Os Estados que não possuem vendedores cadastrados são: Alagoas, Amapá, Tocantins, Roraima. No entanto, quando olhamos para os clientes (compradores) vimos que todos os 26 Estados mais o DF estão contidos nos dados.

Ainda sobre os dados regionais, as 5 cidades (de um total de 599) com mais vendas na base (Top Ranking) são: 

Ranking | Cidade            | Soma das Vendas (R$) 
--------|-------------------|---------------------
1       | São Paulo         | 2.935.165,17
2       | Ibitinga          | 809.908,51
3       | Curitiba          | 519.757,39
4       | Rio de Janeiro    | 376.027,63
5       | Guarulhos         | 347.864,46


Há também um crescimento constante com o evoluir dos meses, apresentando um pico nas vendas no mês de Novembro, provavelmente atribuído ao evento da Black Friday (O mês apresenta também a maior proporção de reclamações, 37%).

<br>

Se tratando da adesão de novos vendedores e clientes, fica observado uma oscilação quando olhamos para a quantidade de novos vendedores a cada mês, e um constante aumento em novos clientes que passaram a utilizam a plataforma da empresa.

Quando analisado apenas as vendas distintas e entregues de 2017, algumas informações saltam na tela como: Janeiro trás a maior *média por pedido*, Outubro por sua vez apresenta a *maior mediana*, Novembro ficando tanto com a *maior soma dos pedidos* quanto com a *maior quantidade de itens vendidos*.
Inclusive fica observado que a maior parte das vendas ocorre entre as 10:00 e as 22:00 horas.

Mês      | Estatísticas Top 1       | Valor da estatística
---------|--------------------------|---------------------
Janeiro  | Média por pedido         | 172,73
Outubro  | Mediana dos pedidos      | 104,03
Novembro | Soma das vendas          | 1.062.700,18
Novembro | Soma de Itens Vendidos   | 6.561

<br>

Uma relação de duas variáveis encontrada nos dados tem haver com a *soma total de vendas de cada Estado* e o seu *valor médio de frete*. Com auxílio do gráfico observamos uma possível correlação negativa de nível moderado entre as duas características (com correlação de Pearson de -0.58)

<br>

O ticket médio (quantidade média de produtos por pedido) não parece apresentar uma tendência de crescendo ao longo do tempo em nenhuma das 5 regiões do país.
Por falar nas regiões, o Norte apresenta a maior porcentagem de frete em relação ao custo total do pedido, representando cerca de 15% de toda a compra (apresenta também a menor soma de vendas, e essa relação negativa se mantém conforme explicado acima).

<br>

A distribuição das notas de avaiação dos pedidos mostra a nota máxima de 5 em aproximadamente 60% dos pedidos. A pior avaliação (1) fica com 9,74%.
Neste quesito os Boletos são a forma de pagamento com maiores reclamações.

Avaliações | % de Avaliações
-----------|----------------
Ótima      | 59.20
Boa        | 19.60
Normal     | 8.30
Ruim       | 3.14
Péssima    | 9.74

<br>

Submetidos os dados a um modelo de Random Forest usando o método de permutação vimos que as características físicas dos produtos possuem alta relevância entre àqueles produtos Top Ranking de vendas, enquanto as variáveis financeiras (como preço ou frete) possuem pouca importância. É possível observar também que feactures informativas como a *descrição* e o *nome do produto* são mais importantes neste caso do que a quantidade de fotos ou o preço.

<br>

Por fim, quando analisado a precisão (acurácia) na data prevista de entrega disponibilizada no ato da compra, vimos mesmo com uma margem de 7 dias (caso o site erre em 7 dias para mais ou para menos, ainda assim é contado como acerto)
o sistema de predição erra em aproximadamente 70% dos pedidos.

</html>

### Dicionário dos dados

Dicionário dos dados

df_customer
* ##### customer_id = ID do cliente
* ##### customer_unique_id = ID único do ciente
* ##### customer_zip_code_prefix = Prefixo do código postal do cliente
* ##### customer_city = Cidade do cliente
* ##### customer_state = Estado do cliente


df_seller
* ##### seller_id = ID do vendedor
* ##### seller_zip_code_prefix = Prefixo do código postal do vendedor
* ##### seller_city = Cidade do vendedor
* ##### seller_state = Estado do vendedor


df_pedidos
* ##### order_id = ID do pedido
* ##### customer_id = ID do cliente
* ##### order_purchase_timestamp = Data e hora da realização do pedido
* ##### order_approved_at = Data e hora da confirmação do pedido
* ##### order_delivered_carrier_date = Data e hora da chegada na transportadora
* ##### order_delivered_customer_date = Data e hora da entrega do produto para o cliente
* ##### order_estimated_delivery_date = Data e hora estimada para a entrega 


df_produtos
* ##### product_id = ID do produto
* ##### product_category_name = Nome da categoria do produto
* ##### product_name_lenght = Tamanho do nome do produto
* ##### product_description_lenght = Tamanho da descrição do produto
* ##### product_photos_qty = Quantidade de fotos do produto
* ##### product_weight_g = Peso do produto
* ##### product_length_cm = Comprimento do produto
* ##### product_height_cm = Altura do produto
* ##### product_width_cm = Largura do produto


df_categoria_prod
* ##### product_category_name = Nome da categoria do produto
* ##### product_category_name_english = Nome da categoria do produto em inglês


df_itens
* ##### order_id = ID do pedido
* ##### order_item_id = Ordem do item no pedido
* ##### product_id = ID do produto
* ##### seller_id = ID do vendedor
* ##### shipping_limit_date = Data limite de envio
* ##### price = Preço do pedido
* ##### freight_value = Valor do frete


df_pagamentos
* ##### order_id = ID do pedido
* ##### payment_sequential = Pagamento sequencial **(?)**
* ##### payment_type = Tipo do pagamento
* ##### payment_installments = Parcelas no pagamento
* ##### payment_value = Valor do pagamento


df_localizacao
* ##### geolocation_zip_code_prefix = Prefixo do código postal
* ##### geolocation_lat = Latitude
* ##### geolocation_lng = Longitude
* ##### geolocation_city = Cidade
* ##### geolocation_state = Estado


df_reviews
* ##### review_id = ID do review
* ##### order_id = ID da venda
* ##### review_score = Nota final
* ##### review_comment_title = Título do comentário
* ##### review_comment_message = Conteúdo da mensagem
* ##### review_creation_date = Data da criação da mensagem
* ##### review_answer_timestamp = Data e hora do timestemp de resposta **(?)**

### Importando os dados
1. [✓] Importações e Filtros

**Importando as bibliotecas de funções necessárias para a Análise Exploratória**
"""

!pip install pandasql

# Cálculos e Métricas
import numpy as np 
from datetime import datetime, timedelta # Manipulação de datas e horas

# Manipulação de dados
import pandas as pd # Manipulação de dados através de DataFrames
import pandasql as ps # Consulta aos DataFrames em SQL
pd.set_option('display.max_columns', None) # Mostrar todas as colunas dos dataframes
pd.options.display.float_format = '{:.2f}'.format # Mostrar apenas duas casas decimais para números floats

# Visualização dos dados
import matplotlib.pyplot as plt # Plotagem de gráficos
from matplotlib.pyplot import figure
import plotly.express as px # Plotagem de gráficos interativos
import seaborn as sns
plt.style.use('fivethirtyeight')
import missingno as msno # Plotagem de valores ausentes
import folium # Plotagem de gráficos geoespaciais
from folium import plugins
from plotly.subplots import make_subplots
import plotly.graph_objects as go

# Usado para indução de relevância de características
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance

# Outros
import warnings # Ignorando mensagens de aviso
warnings.filterwarnings('ignore')

print('Bibliotecas e Configurações Prontas')

"""**Unindo todos os dataframes em um único**"""

files = {'customers':      '/content/drive/MyDrive/Github/SQL/Dados/Olist/data/olist_customers_dataset.csv',
         'sellers':        '/content/drive/MyDrive/Github/SQL/Dados/Olist/data/olist_sellers_dataset.csv',
         'order_reviews':  '/content/drive/MyDrive/Github/SQL/Dados/Olist/data/olist_order_reviews_dataset.csv',
         'order_items':    '/content/drive/MyDrive/Github/SQL/Dados/Olist/data/olist_order_items_dataset.csv',
         'products':       '/content/drive/MyDrive/Github/SQL/Dados/Olist/data/olist_products_dataset.csv',
         #'geolocation':    '/content/drive/MyDrive/Github/SQL/Dados/Olist/data/olist_geolocation_dataset.csv',
         'orders':         '/content/drive/MyDrive/Github/SQL/Dados/Olist/data/olist_orders_dataset.csv',
         'order_payments': '/content/drive/MyDrive/Github/SQL/Dados/Olist/data/olist_order_payments_dataset.csv'
}

data = {}
for key, value in files.items():
    data[key] = pd.read_csv(value, sep=',', encoding='utf-8')

"""**Realizando os joins, tendo em vista os Sellers**"""

sellers_full = data['sellers'].merge(data['order_items'], on='seller_id')
sellers_full = sellers_full.merge(data['orders'], on='order_id')
sellers_full = sellers_full.merge(data['products'], on='product_id')
sellers_full = sellers_full.merge(data['order_payments'], on='order_id')
sellers_full = sellers_full.merge(data['order_reviews'], on='order_id')
sellers_full = sellers_full.merge(data['customers'], on='customer_id')
dataf = sellers_full.copy()

"""**Selecionando as colunas desejadas para a análise.**

**Em seguida, convertendo as colunas com data e hora para o formato correto.**

**E por fim, resetando os índices para manter o dataset organizado.**
"""

# Filtrando
dataf = dataf[['seller_id','order_id', 'order_status','order_purchase_timestamp','order_approved_at',
 'order_delivered_carrier_date', 'order_delivered_customer_date',
 'order_estimated_delivery_date','shipping_limit_date', 'price',
 'freight_value', 'payment_value','payment_sequential','payment_type','product_id','product_category_name',
 'payment_installments','customer_id','customer_unique_id','customer_city',
 'customer_state','product_name_lenght','product_description_lenght', 'product_photos_qty',
 'product_weight_g','product_length_cm', 'product_height_cm', 
 'product_width_cm','seller_zip_code_prefix', 'seller_city', 'seller_state',
 'review_id', 'review_score', 'review_comment_title','review_comment_message',
 'review_creation_date','review_answer_timestamp']]

# Convertendo as colunas
cols_datetime = ['order_approved_at','order_purchase_timestamp',
            'order_delivered_carrier_date','order_delivered_customer_date',
            'order_estimated_delivery_date','shipping_limit_date',
            'review_creation_date','review_answer_timestamp']
for col in cols_datetime:
    dataf[col] = pd.to_datetime(dataf[col])

# Resetando os índices
dataf = dataf.reset_index(drop=True)

"""### Área das funções
1. [✓] Funções

**Área destinada a criação das funções que serão utilizadas no decorrer da exploração**
"""

# Apresenta a quantidade e porcentagem apenas das colunas que contém algum valor nulo
def qtd_nulos(dataf):
  df_null = pd.DataFrame(dataf.isnull().sum(),columns=['Qtd_nulos'])
  df_null['%_nulos'] = (dataf.isnull().sum()/len(dataf))*100
  return df_null.loc[df_null.Qtd_nulos > 0].sort_values(by='Qtd_nulos', ascending=False)

# Apresenta estatísticas sobre as notas de avaliação dos pedidos (nulos e não nulos)
def dist_null(df,coluna):
  null_desc = dataf.loc[dataf[coluna].isnull() == True][['review_score']].describe()
  null_desc[''] = dataf.loc[dataf[coluna].isnull() != True][['review_score']].describe()
  null_desc.columns = ['Nulos', 'Não Nulos']
  return null_desc

# Ref: https://stackoverflow.com/questions/38783027/jupyter-notebook-display-two-pandas-tables-side-by-side
# Plota mais de um dataframe um ao lado do outro
from IPython.core.display import display, HTML
def mostra_dfs_lado_lado(dfs:list, captions:list):    
    output = ""
    combined = dict(zip(captions, dfs))
    for caption, df in combined.items():
        output += df.style.set_table_attributes("style='display:inline'").set_caption(caption)._repr_html_()
        output += "\xa0\xa0\xa0"
    display(HTML(output))

# Imputando valores quantitativos pela moda
def imput_quant(dataf, columns):
  for col in columns:
    dataf[col].fillna(dataf[col].value_counts().index[0], inplace=True)

# Plotando novos cadastros por mês
def novos_cadastros(df,coluna):
  novos_cadastros = df.loc[df['order_status'] == 'delivered'][[coluna,'order_approved_at']].groupby(coluna).min()
  novos_cadastros['order_approved_at'] = novos_cadastros['order_approved_at'].apply(lambda x: str(x)[:-12]) # Aplicando Slicing e deixando apenas o mês e o ano da coluna
  X = novos_cadastros['order_approved_at'].value_counts().index
  Y = novos_cadastros['order_approved_at'].value_counts().values

  if (coluna == 'customer_unique_id'):
    cadastros = 'Clientes'
  if (coluna == 'seller_id'):
    cadastros = 'Vendedores'

  fig = px.bar(novos_cadastros, x=X, y=Y, labels=dict(x = 'Meses', y = cadastros),width=800, height=450)
  fig.update_layout(title={
      'text':'Novos '+ cadastros + ' por Mês',
      'x':0.5,
      'xanchor':'center'})
  return fig

# Plotando comparativo de ticket médio por regiões (versão temporária da função)
def plot_ticket(df, regioes):
  fig = make_subplots(rows=2, cols=3)

  reg_ticket_plot = df.query(f'regiao_x == "{regioes[0]}"')
  fig.append_trace(go.Bar(
      x=reg_ticket_plot['order_approved_at'],
      y=reg_ticket_plot['ticket_medio'],
      text=round(reg_ticket_plot['ticket_medio'],2),
      name=f'{regioes[0]}'),
      row=1, col=1)
  
  reg_ticket_plot = df.query(f'regiao_x == "{regioes[1]}"')
  fig.append_trace(go.Bar(
      x=reg_ticket_plot['order_approved_at'],
      y=reg_ticket_plot['ticket_medio'],
      text=round(reg_ticket_plot['ticket_medio'],2),
      name=f'{regioes[1]}'),
      row=1, col=2)
  
  reg_ticket_plot = df.query(f'regiao_x == "{regioes[2]}"')
  fig.append_trace(go.Bar(
      x=reg_ticket_plot['order_approved_at'],
      y=reg_ticket_plot['ticket_medio'],
      text=round(reg_ticket_plot['ticket_medio'],2),
      name=f'{regioes[2]}'),
      row=1, col=3)
  
  reg_ticket_plot = df.query(f'regiao_x == "{regioes[3]}"')
  fig.append_trace(go.Bar(
      x=reg_ticket_plot['order_approved_at'],
      y=reg_ticket_plot['ticket_medio'],
      text=round(reg_ticket_plot['ticket_medio'],2),
      name=f'{regioes[3]}'),
      row=2, col=1)
  
  reg_ticket_plot = df.query(f'regiao_x == "{regioes[4]}"')
  fig.append_trace(go.Bar(
      x=reg_ticket_plot['order_approved_at'],
      y=reg_ticket_plot['ticket_medio'],
      text=round(reg_ticket_plot['ticket_medio'],2),
      name=f'{regioes[4]}'),
      row=2, col=2)
  fig.update_layout(title={"text":"Ticket Médio por Região (Jan2017 à Ago2018)", "x":0.5}, showlegend=False)
  fig.update_yaxes(range=[0,max(reg_ticket.ticket_medio)])
  fig.update_xaxes(title_text=f"{regioes[0]}", row=1, col=1)
  fig.update_xaxes(title_text=f"{regioes[1]}", row=1, col=2)
  fig.update_xaxes(title_text=f"{regioes[2]}", row=1, col=3)
  fig.update_xaxes(title_text=f"{regioes[3]}", row=2, col=1)
  fig.update_xaxes(title_text=f"{regioes[4]}", row=2, col=2)
  fig.show()

# Correlações entre as variáveis
def correlacoes(df):
  corr = df.corr() # Gerando a tabela de correlação
  mask = np.zeros_like(corr, dtype=bool) # Criando uma matrix de valores booleanos, onde os dados só aparecerão, caso o valor seja False
  mask[np.triu_indices_from(mask)] = True

  fig, aux = plt.subplots(figsize=(12,11))

  # gerando a palleta de cores
  cmap = sns.diverging_palette(240, 10, as_cmap=True)

  sns.heatmap(
      corr,          
      mask=mask,     # mascara triangular de True/False
      cmap=cmap,     # palleta de cores
      annot=True,    # plotar os valores dentro das células
      square=True,   # Forçar células a serem quadradas
      linewidths=.5, # Largura das linhas que dividem as células
      cbar_kws={"shrink": .5})
  fig.show();

# Função recebe o df e retorna ele normalizado.
# Vou apenas normalizar os dados quantitativos, pois vou usar uma Random Forest para a indução de importância das características.
def normaliza(df, flag_remove):
  cols_num = df.select_dtypes(include=np.number).columns.tolist()
  cols_num.remove(flag_remove)
  df = df[cols_num]
  df = (df - df.min()) / ( df.max() - df.min())
  return df

# Função recebe o df noraliza e retorna ele pronto para o modelo.
def concatena_norm(df, flag_remove):
  df_num_norm = normaliza(df, flag_remove)
  #df_object = df.select_dtypes(include=np.object_)
  #df_normalizado = pd.concat([df_num_norm,df_object,df[flag_remove]], axis=1)
  df_normalizado = pd.concat([df_num_norm,df[flag_remove]], axis=1)
  return df_normalizado

# Função que recebe o df normalizado e retorna as características mais relevantes por meio de Permutação.
def relevancia(df, flag):
  # Selecionando as colunas e os dados
  feature_names = df.columns.tolist()
  feature_names.remove(flag)
  X = df[feature_names]
  y = df[flag]

  # Separando os dados
  X_train, X_test, y_train, y_test = train_test_split(X, y , stratify=y, random_state=42)

  # Instanciando e treinando o modelo
  forest = RandomForestClassifier(random_state=0)
  forest.fit(X_train, y_train)

  # Calculando a importância e o Desvio Padrão de cada característica
  result = permutation_importance(forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)
  forest_importances = pd.Series(result.importances_mean, index=feature_names)
  std = result.importances_std

  # Criando e retornando um DataFrame e um Gráfico das informações
  forest_importances.sort_values(ascending=False,inplace=True)
  fig = px.bar(forest_importances, error_y=std, width=800, height=450, text_auto='.2s')
  fig.update_layout(title={"text":"Importância das Características dos Produtos", "x":0.5},showlegend=False)
  return fig, forest_importances

"""### Visualizando os dados
1. [✓] Visualizando os dados e seus tipos

Visualizando uma amostra dos dados (os 5 primeiros e últimos registros)
"""

dataf

"""**Visualizando as colunas e seus tipos de dados**"""

dataf.info()

"""### Valores Duplicados
1. [✓] Remoção de linhas duplicadas
"""

#Remoção de linhas duplicadas#
print('Foram removidas {} linhas duplicadas'.format(len(dataf[dataf.duplicated()])))
dataf.drop_duplicates(keep=False, inplace=True)

"""### Resumo dos dados
1. [✓] Resumo de valores únicos nas tabelas
2. [✓] Resumo das principais estatísticas dos dados
"""

# Quantidade/Soma de valores únicos das colunas selecionadas abaixo
uniq_cols = ['seller_id', 'customer_unique_id', 'order_id', 'product_id', 'seller_city', 'customer_city', 'seller_state', 'customer_state']
l = {}
for col in uniq_cols:
  l[dataf[col].name] = len(dataf[col].unique())

vals_unicos = pd.DataFrame.from_dict(l, orient='index',columns=['Qtd Valores Únicos'])
vals_unicos

"""Analisando as principais características qualitativas, podemos observar algumas informações interessantes, como:

1. Em `seller_id`, o vendedor que mais vendeu na base, possui 1794 vendas distintas (na tabela abaixo aparece 1988 vendas pois existem vendas com mais de um ítem no pedido.)

2. Já em `order_id` podemos ver que o pedido em que mais teve itens adicionados ao carrinho possui 21 itens. Analisando mais de perto a venda, pude observar que a mesma não foi entregue, teve uma nota de avaliação 1 (um) e o cliente realizou apenas esta compra na base, e depois disso não comprou mais (é o mesmo cliente que aparece em `customer_id` e `customer_unique_id`). Já analisando o perfil do comprador é possível notar que ele realizou no total 8 vendas distintas, possui uma média de 4,25 nas avaliações e vende ferramentas de jardinagens e itens de construção para casa.

3. Em `order_status`, como o esperado a classe *delivered* (entregue) é a classe majoritária com 98% das vendas, seguida de *shipped* (enviado) com 1%.

4. Cartões de Crédito e Boletos são os dois pagamentos com maior frequência com 75% e 19% respectivamente.

5. `cama_mesa_banho` é a categoria mais frequênte (dentre as outras 72), com 9.4% de todas as vendas (distintas).

6. Os Estados onde não possuem vendedores cadastrados são: Alagoas, Amapá, Tocantins, Roraima.
"""

dataf.describe(include='object')

"""Já analisando as estatísticas das características quantitativas, podemos ver que:

Existem algumas características (`freight_value`,`payment_value`,`payment_installments` e `product_weight_g`) com 0 (zero) como valores, indicando possivelmente um erro. Por se tratar de poucas observações e para fins de praticidade, irei removê-las da base.

Diversas características (`freight_value`,`payment_value`,`payment_installments`, etc) apresentam outliers, a priori eles permanecerão na base pois vou aplicar os dados à modelos que os suportam. Posteriormente os removerei para fins de comparação de desempenho. 
"""

dataf.describe()

"""---

### Dados Ausentes
1. [✓] Visualizando missing
2. [✓] Explicando e imputando os missing

**Visualizando a quantidade de valores ausentes em cada coluna**
"""

msno.matrix(dataf,sort='ascending', figsize=(9,5), fontsize=(10), filter=[dataf.isnull().sum() > 0]);

"""Visualizando a quantidade e a porcentagem dos valores ausentes

• Apenas as informações de *título do comentário* (`review_comment_title`) e *conteúdo da mensagem de avaliação* (`review_comment_message`) possuem quantidade significativa de valores ausentes.

• Poderíamos certamente extrair muitas informações importantes desses dados, caso estivessem presentes. No entanto, para esta etapa exploratória do projeto, essas informações permanecerão da forma em que estão.

• Para a coluna `product_category_name` os valores faltantes serão preenchidos com o valor 'Outros'.

• Para aqueles dados quantitativos (contínuos e discretos) imputarei com o valor mais frequente.

• E finalmente para os valores de *datas e horas*, apenas excluirei esses registro. No entando, irei deixá-los por enquanto, para que eu possa analisá-los melhor mais abaixo.
"""

# Quantidades e % de Missing Values
qtd_nulos(dataf)

"""**Imputando os valores ausentes**"""

# product_category_name
dataf.loc[dataf.product_category_name.isnull() == True, 'product_category_name'] = 'Outros'

# Dados quantitativos
colunas_miss = ['product_name_lenght','product_description_lenght','product_photos_qty','product_weight_g','product_length_cm','product_height_cm','product_width_cm']
imput_quant(dataf, colunas_miss)

"""---

### Correlações
1. [✓] O que as correlações nos dizem sobre os dados?

Na matrix de correlação abaixo, não pude encontrar muitas informações relevantes (devido a minha pouca experiência na área).

No entanto a característica `price` será removida da base, devido sua
alta correlação com o `payment_value` (para não impedir os modelos de regressão de realizarem *Ceteris Paribus*).

Se tratando de características físicas (*tamanho e peso*) já é esperado que haja uma correlação como visto abaixo.

O valor do frete estar relacionado com o peso do produto também é outra coisa já esperado.


p-valor (+ ou -)| Interpretação
----------------|--------------
0.00 à 0.19     | Bem Fraca
0.20 à 0.39     | Fraca
0.40 à 0.69     | Moderada
0.70 à 0.89     | Forte
0.90 à 1.00     | Muito Forte
"""

# Correlações entre as variáveis
correlacoes(dataf)

# Removendo a coluna citada acima
dataf.drop('price', axis=1, errors='ignore', inplace=True)

"""### Outliers
1. [✓] Encontrar e tratar os outliers

O tratamento dos outliers será feito a medida em que forem encontrados e houver a extrema necessidade de remoção, visto que, alguns dos algoritmos que aprenderá com os dados, serão a *Regressão Quantílica* e *Random Forest*.

---

### Empresa
1. [✓] As vendas estão aumentando com o passar do tempo (Considerar apenas vendas entregues e distintas)? 
2. [✓] Quantos novos clientes em cada mês/ano?
3. [✓] Quantos novos vendedores em cada mês/ano?
4. [✓] Quais os meses de maiores vendas (Considerar apenas o ano de 2017)?
5. [✓] Quais horários com maiores vendas?
6. [✓] Quais os pagamentos são mais frequêntes? E quando o pedido não é entregue?

**As vendas estão aumentando com o passar do tempo (Considerar apenas vendas entregues e distintas)?**

Com base na soma de vendas por período, aparentemente, há uma tendência de aumento (principalmente no mês de Novembro de 2017 como sinalizado no gráfico) nas vendas da Olist.

Nos dados mostrados abaixo, decidi não mostrar as vendas de 2016, visto que, apresentam poucos pedidos disponíveis no dataset analisado (242 pedidos em Outubro e 1 em Dezembro)

Embora não seja possível observar presença de sazonalidade, houve um nítido aumento no volume de vendas entre 2017 e 2018. Sinais de que a Olist, está em ritmo de crescimento acelerado e consistente (com base nos dados disponíveis).
"""

query = """
SELECT strftime('%m/%Y',order_approved_at) AS data,
      COUNT(DISTINCT(order_id)) AS qtd_vendida
FROM dataf
WHERE order_status = 'delivered' AND
      order_approved_at IS NOT NULL AND
      CAST(STRFTIME('%Y',order_approved_at) AS INT) >= 2017
GROUP BY data
"""

vendas_period = ps.sqldf(query)
vendas_period['data'] = pd.to_datetime(vendas_period['data'])
vendas_period.loc[vendas_period['data'] == '2017-11-01', 'datas_importantes'] = 'Black Friday'
vendas_period = vendas_period.sort_values('data')

fig_vendas_tmp = px.line(vendas_period, x="data", y="qtd_vendida",width=800, height=450, text="datas_importantes")
fig_vendas_tmp.update_layout(
    title={
        'text': "Total de Vendas por Período",
        'x':0.5,
        'xanchor': 'center'},
        hovermode="x")
fig_vendas_tmp.show()

"""---

**Quantos novos clientes em cada mês/ano?**

Indo na mesma direção do gráfico anterior (total de vendas por período), podemos observar uma crescente nos números de pedidos realizados por clientes distintos, indicando um aumento na adesão de novos clientes ao longo do tempo.

==========================================================

**Quantos novos vendedores em cada mês/ano?**

Numa direção diferente apresentada no gráfico de novos clientes por mês, a quantidade de novos vendedores ao longo do tempo não apresenta um aumento quase constante, é visível uma oscilação, porém, devido minha falta de experiência não fica claro apenas com essas informações quais as possíveis causas para tal.
"""

# Novos Clientes
cad_clientes = novos_cadastros(dataf,'customer_unique_id')
# Novos Vendedores
cad_vendedores = novos_cadastros(dataf,'seller_id')

cad_clientes.show(), cad_vendedores.show()

"""---

**Quais os meses de maiores vendas? Considerar apenas o ano de 2017.**

Aqui, nas informações apresentadas abaixo, podemos ver que apesar do mês de Novembro ser o mês de maior venda (provavelmente devido a *Black Friday*) o mês de Abril possui a maior média de venda. 

Porém, quando olhamos com atenção à mediana (median), podemos notar que o mês de Maio é aquele que possui um valor maior, valor este que separa os dados em dois grupos de tamanhos iguais.

E os primeiros meses do ano representando uma baixa nas vendas/compras como é esperado no varejo devido às compras do final de ano (que não por coincidência, são os meses em que ocorreram mais vendas).
"""

# Agrupamento e soma das vendas (distintas) de 2017
vendas_meses = dataf.loc[(dataf['order_status'] == 'delivered') & (dataf['order_approved_at'].dt.year == 2017)]\
                    .groupby([dataf['order_id'], dataf['order_approved_at'].dt.month])\
                    .agg(['sum'])['payment_value'].reset_index() 
del vendas_meses['order_id']
vendas_meses.columns = ['order_approved_at', 'payment_value']

# Agrupando por meses
vendas_meses = vendas_meses.groupby('order_approved_at').agg({'payment_value':['sum','count','mean','median','max']})
vendas_meses.style.highlight_max(color='green')

"""---

**Quais horários com maiores vendas?**

Conforme podemos observar abaixo, os horários de maiores movimentos (compras/vendas) estão entre 11:00 e 22:00 e com retomada a partir das 06:00.

O que é natural, analisando o perfil dos compradores em geral.
"""

# Filtrando Vendas entregues
vendas_ok = dataf.loc[(dataf['order_status'] == 'delivered') & (dataf['order_approved_at'].dt.year == 2017)]

# Venda parcelada em 0 vezes? (Venda entregue e com 5 de review, possível erro no sistema)
vendas_ok.loc[vendas_ok['payment_installments'] == 0, 'payment_installments'] = 1

# Vendas distintas e a sua hora
vendas_horas = ps.sqldf("SELECT DISTINCT(order_id), strftime('%H',order_purchase_timestamp) as Horas FROM vendas_ok")['Horas'].value_counts().reset_index()
vendas_horas.columns = ['Horas','Qtd_vendas']
vendas_horas.sort_values('Horas', inplace=True)

fig_horarios = px.bar(vendas_horas,x='Horas', y='Qtd_vendas', width=800, height=450)
fig_horarios.show()

"""---

**Quais os pagamentos são mais frequêntes? E quando o pedido não é entregue?**

Os tipos de status do pedido (`order_status`) *unavailable* (indisponível) e *approved* (aprovado) possuem apenas a classe credit_card.

Todos os outros tipos de status, seguem uma distribuição incrivelmente parecida com aquela mostrada abaixo.
"""

freq_pag = vendas_ok['payment_type'].value_counts(normalize=True)
freq_pag

"""---

### Regiões
1. [✓] Quanto cada região do país representa em % das vendas?
2. [✓] Distribuição de parcelamentos dos pedidos (dos Estados e Regiões).
3. [✓] Soma de Vendas por Estado
4. [✓] Quais são os estados em que mais possuem vendas e qual o seu valor médio de frete?
5. [✓] Ticket médio por região tem aumentado com o tempo?
6. [✓] Para cada Estado (e região), o frete representa quantos % do valor total?

Fazendo Join para trazer as Regiões para a base de dados
"""

# Fazendo Join
reg = pd.read_csv('https://raw.githubusercontent.com/kelvins/Municipios-Brasileiros/main/csv/estados.csv')
reg_merge = dataf.merge(reg[['uf','regiao']], left_on='customer_state', right_on='uf')
dataf_reg = reg_merge.merge(reg[['uf','regiao']], left_on='seller_state', right_on='uf')
vendas_ok = dataf_reg.query('order_status == "delivered" ')

"""**Quanto cada região do país representa em % das vendas?**"""

vendas_regiao = pd.DataFrame(vendas_ok.groupby('regiao_y').agg('sum')['payment_value'])
soma_vend_tot = vendas_ok.payment_value.sum()
vendas_regiao['% Vendas'] = vendas_regiao['payment_value'].apply(lambda x: round((x / soma_vend_tot),4) * 100)
vendas_regiao = vendas_regiao.sort_values('% Vendas', ascending=False).reset_index()
vendas_regiao

fig_vendas_regiao = px.bar(vendas_regiao,
       x='payment_value',
       y='regiao_y',
       orientation='h',
       category_orders={"regiao_y":vendas_regiao.regiao_y},
       text_auto='.5s',
       labels={'regiao_y': 'Regiões',
               'payment_value': 'Soma de Vendas'},
       width=800,
       height=450)

fig_vendas_regiao.update_layout(title={"text":"Total de Vendas por Região",
                         "x":0.5})
fig_vendas_regiao.show()

"""---

**Distribuição de parcelamentos dos pedidos (dos Estados e Regiões)**
"""

# Vendas e suas parcelas
vendas_parc = vendas_ok[['order_id','payment_installments']]
vendas_parc = vendas_parc.groupby('order_id').max()

# Separando os pedidos em dois grupos, a partir da quantidade de parcelas
parc = 4
vend_parc_baix = vendas_parc[vendas_parc <= parc].dropna() # ID dos pedidos com 4 parcelas ou menos
vend_parc_altas = vendas_parc[vendas_parc > parc].dropna() # ID dos pedidos com mais de 4 parcelas

# Qual a Proporção dos dois grupos?
qtd_total = len(vendas_parc)

qtd_baixa = len(vend_parc_baix)
porc_baixa = round(qtd_baixa / qtd_total,3) * 100

qtd_altas = len(vend_parc_altas)
porc_altas = round(qtd_altas / qtd_total,3) * 100

print(f'Parcelas  <= {parc}: {qtd_baixa} ({porc_baixa}%)\nParcelas   > {parc}: {qtd_altas} ({porc_altas}%)')

# Os estados dos clientes e seus parcelamentos
box_parc = vendas_ok[['order_id','customer_state','regiao_x','payment_type','payment_installments']]

box_parc_baixa = box_parc.query(f'payment_installments <= {parc}') # Pedidos com 4 parcelas ou menos
box_parc_alta = box_parc.query(f'payment_installments > {parc}')   # Pedidos com mais de 4 parcelas

# Algum Estado (ou o DF) está presente em apenas um dos dois grupos?
grp_baixo = len(box_parc_baixa.customer_state.unique())
grp_alto = len(box_parc_alta.customer_state.unique())
grp_baixo,grp_alto

# Distribuição de Parcelas (dos Pedidos) por Estado e Grupo
fig_est = make_subplots(rows=1, cols=1)

# Qual a distribuição dos Estados muitos parcelamentos?
grf1 = go.Box(x=box_parc_alta['customer_state'],y=box_parc_alta['payment_installments'], name=f'Parcelas > {parc}')
fig_est.append_trace(grf1, row=1, col=1)

# E com poucos parcelamentos?
grf2 = go.Box(x=box_parc_baixa['customer_state'],y=box_parc_baixa['payment_installments'], name=f'Parcelas <= {parc}')
fig_est.append_trace(grf2, row=1, col=1)

fig_est.update_layout(title={'text':"Distribuição de Parcelas por Estado (e Grupo)",
                             'x':0.5})
fig_est.show()

"""Quantidades mais frequêntes de **baixas** parcelas por região:

Todas as regiões possuem densidades bem demarcadas nas 4 (quatro) parcelas

Exemplo: Todas as Regiões: [1, 2, 3, 4] (Com a grande maioria em 1x)


==========================================================

Quantidades mais frequêntes de **altas** parcelas por região:

• Sudeste:  [5, 6, 8, 10]

• Norte: [5, 10]

• Sul: [5, 8, 10]

• Nordeste: [5, 8, 10]

• Centro-Oeste: [5, 8, 10]

==========================================================

**Resumo:**

O tratamento de outliers é um processo importante nas etapas de um projeto de Data Science, e simplesmente removê-los sem entender o impacto que isso pode causar na aprendizagem (e generalização) do modelo não me parece algo certo. 
Por esse e outros motivos, irei remover da base, todos os pedidos com parcelas acima de 10x. 

No momento, esta decisão me parece ser a correta.
"""

fig_reg = make_subplots(rows=1, cols=1)

# Qual a densidade e distribuição com muitos parcelamentos?
grf1 = go.Violin(x=box_parc_alta['regiao_x'],y=box_parc_alta['payment_installments'], name='Densidade e Distrib. Parcelas >= 4')
fig_reg.append_trace(grf1, row=1, col=1)

# E com poucos parcelamentos?
grf2 = go.Violin(x=box_parc_baixa['regiao_x'],y=box_parc_baixa['payment_installments'], name='Densidade e Distrib. Parcelas < 4')
fig_reg.append_trace(grf2, row=1, col=1)

fig_reg.update_layout(title={'text':"Distribuição de Parcelas por Região (e Grupo)",
                             'x':0.5})
fig_reg.show()

"""

---

"""

# Quais as médias de avaliação por Estado (Estados mais bem avaliados)?
reg_aval = vendas_ok.groupby('seller_state').agg([pd.Series.mode,'mean','count'])['review_score'].reset_index()
reg_aval

# Carregar o arquivo de geolocalização json
state_geo = 'https://raw.githubusercontent.com/datalivre/Conjunto-de-Dados/master/br_states.json'

# Criar o mapa base
mapa = folium.Map(location=[-15.77972, -47.92972], zoom_start=5)
# Criar a camada Choroplet
folium.Choropleth(
    geo_data=state_geo,
    data=reg_aval, # meus dados
    columns=['seller_state', 'mean'], # o código postal está aqui para corresponder ao código postal geojson, o preço de venda é a coluna que altera a cor das áreas do código postal
    key_on='feature.id', # este caminho contém códigos postais no tipo str, esses códigos ID correspondem à sigla do Estado
    fill_color='YlGnBu', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Médias das avaliações',
    smooth_factor=0,
    Highlight= True,
    line_color = "#0000",
    name = "mean",
    show=False,
    overlay=True,
    nan_fill_color = "White"
).add_to(mapa)

# Visualizar
#mapa

mapa

"""---

**Quais são os estados em que mais possuem vendas e qual o seu valor médio de frete?**

Podemos observar uma nítida diferença na distribuição de vendas nos estados. Ao mesmo tempo que observa-se que aqueles estados com mais vendas parecem possuir uma média menor no frete.
"""

query = """
select customer_state as estado,
       count(distinct(order_id)) as qtd_vendas,
       (sum(freight_value) / count(distinct(order_id))) as media_frete

from dataf
group by estado
order by qtd_vendas
"""

comp_frete_estado = ps.sqldf(query)
comp_frete_estado

# Parece haver uma correlação negativa entre o valor médio do frete e a quantidade de vendas realizadas naquele Estado (de fato há, -0.58 (Pearson)).
# Estados com fretes mais baratos costumam ter mais vendas.
fig_vendas_frete_estado = make_subplots(rows=2, cols=1)

fig_vendas_frete_estado.append_trace(go.Scatter(
    x=comp_frete_estado['estado'],
    y=comp_frete_estado['qtd_vendas'],
    name='Qtd Vendas'),
    row=1, col=1)

fig_vendas_frete_estado.append_trace(go.Scatter(
    x=comp_frete_estado['estado'],
    y=comp_frete_estado['media_frete'],
    name='Valor Médio do Frete'),
    row=2, col=1)

fig_vendas_frete_estado.update_layout(height=500, width=900, title={"text":"Quantidade de Vendas X Valor Médio do Frete", "x":0.5})
fig_vendas_frete_estado.show()

comp_frete_estado.corr() # Nível de correlação: Moderada

"""---

**Ticket médio por região tem aumentado com o tempo?**

Fórmula do Ticket Médio: Faturamento total / Número de Vendas

======================================================

**Conclusão**

Centro Oeste: Parece não haver aumento com o tempo, porém, com presença de alta em Fevereiro.

Nordeste: Não parece haver tendência de alta.

Norte: Valores constantes ao longo do tempo com a presença de 3 altas, 1 (uma) em Fevereiro e 2 (duas) em Agosto.

Sudeste: Leve aumento com o passar do tempo, sem presença de altas.

Sul: Não parece haver uma tendência de alta, porém é possível vermos repetição (de queda e baixa) nos valores a cada 4 ou 6 meses.
"""

# Agrupando as informações de vendas e região (a partir de 2017)
reg_ticket = vendas_ok.loc[vendas_ok['order_approved_at'].dt.year >= 2017].groupby(['regiao_x','order_id']).agg(['sum','count'])[['payment_value','freight_value']].reset_index()

# Fazendo join para conseguir a data da venda e renomeando as colunas
reg_ticket = reg_ticket.merge(vendas_ok[['order_id','order_approved_at']], on='order_id') 
reg_ticket.columns = ['order_id','regiao_x','order_id','payment_value_sum','payment_value_count','freight_value_sum','freight_value_count','order_approved_at']
del reg_ticket['freight_value_count']

# Aplicando Slicing e deixando apenas o mês e o ano da coluna e convertendo-a para data
reg_ticket['order_approved_at'] = reg_ticket['order_approved_at'].apply(lambda x: str(x)[:-12]) 
reg_ticket['order_approved_at'] = pd.to_datetime(reg_ticket['order_approved_at'])

# Agrupando por região e data
reg_ticket = reg_ticket.groupby(['regiao_x','order_approved_at']).sum().reset_index()
#
# Criando a coluna ticket médio
faturamento = reg_ticket['payment_value_sum'] + reg_ticket['freight_value_sum']
reg_ticket['ticket_medio'] = faturamento / reg_ticket['payment_value_count']
reg_ticket.head()

regioes = reg_ticket.regiao_x.unique()
plot_ticket(reg_ticket, regioes)

"""---

**Para cada Estado (e região), o frete representa quantos % do valor total?**
"""

# Por Estado
est_pay_feight = vendas_ok.loc[vendas_ok['order_approved_at'].dt.year >= 2017].groupby(['customer_state','order_id']).agg('sum')[['payment_value','freight_value']].reset_index()
est_pay_feight = est_pay_feight.groupby('customer_state').sum()
est_pay_feight['%_frete'] = est_pay_feight['freight_value'] / (est_pay_feight['payment_value'] + est_pay_feight['freight_value']) * 100
est_pay_feight.sort_values('%_frete')

# Por Região
reg_pay_feight = vendas_ok.loc[vendas_ok['order_approved_at'].dt.year >= 2017].groupby(['regiao_x','order_id']).agg('sum')[['payment_value','freight_value']].reset_index()
reg_pay_feight = reg_pay_feight.groupby('regiao_x').sum()
reg_pay_feight['%_frete'] = (reg_pay_feight['freight_value'] / (reg_pay_feight['payment_value'] + reg_pay_feight['freight_value'])) * 100
reg_pay_feight = reg_pay_feight.sort_values('%_frete')
reg_pay_feight

"""---

### Produtos
1. [✓] Qual a receita de cada categoria de produtos entregues? E a média de avaliação?
2. [✓] Quantos produtos em média cada Estado vende em cada pedido distinto realiado?
3. [✓] Quais as características são mais relevantes em relação a venda dos produtos mais vendidos?

**Qual a receita de cada categoria (Top 20 Ranking) de produtos entregues? E a média de avaliação?**

Por mais que a minha inesperiência me impeça de extrair algum possível insight na visualização abaixo, estas informações serão aproveitadas na etapa de coleta de variáveis (Feature Engineering).
"""

# Filtrando colunas e pedidos entregues, agrupando por vendas distintas e por categoria e calculando as estatísticas de cada coluna
prods = dataf_reg.query('order_status == "delivered"')[['product_category_name','order_id','payment_value','freight_value','review_score']].groupby(['order_id','product_category_name']).agg({'review_score':'mean','freight_value':'sum','payment_value':'sum'}).reset_index()

# Agrupando por categoria e calculando as estatísticas de cada coluna
prods = prods.groupby('product_category_name').agg({'review_score':'mean','freight_value':'sum','payment_value':'sum'}).reset_index()

# Criando coluna receita
prods['receita'] = prods['payment_value'] + prods['freight_value']

# Filtrando as colunas, ordenando por receita, selecionando TOP 20 Ranking e aplicando efeito gradiente
prods = prods[['product_category_name','review_score','receita']].sort_values('receita', ascending=False)[:20].style.background_gradient()
prods

"""---

**Quantos produtos em média cada estado vende em cada pedido distinto realiado?**

Adicionei a coluna sum (quantide de pedidos realizados) para que possamos fazer uma comparação quando formos analisar a média.
"""

# Filtrando as colunas desejadas, agrupando por estado e vendas distintas e contando o resultado. Logo em seguida, agrupando por estado novamente e extraindo as estatísticas desejadas (média e soma de product_id)
media_prods_est = dataf_reg[['order_id','product_id','seller_state']].groupby(['seller_state','order_id']).agg('count').reset_index().groupby('seller_state').agg(['mean','sum']).sort_values(by=[('product_id', 'mean')], ascending=False)
media_prods_est

"""---

**Quais as características são mais relevantes em relação a venda dos produtos mais vendidos?**

**Imputação de Relevância das Características utilizando Permutação**

A importância do recurso de permutação supera as limitações do método baseado em impureza
importância do recurso: eles não têm um viés para recursos de alta cardinalidade e podem ser calculados em um *conjunto de teste omitido*.
<a href="https://towardsdatascience.com/from-scratch-permutation-feature-importance-for-ml-interpretability-b60f7d5d1fe9">Fonte</a>

A complexidade computacional para a permutação é mais cara. As características são embaralhadas n vezes e o modelo reajustado para estimar a importância delas.
"""

# Encontrando os produtos mais vendidos
top50 = dataf_reg.query('order_status == "delivered"')['product_id'].value_counts()[:100]
top50 = pd.DataFrame(top50)
produtos = top50.index.tolist()

# Criando uma flag com valor de 1 caso o produto seja top 100 e 0 caso contrário
dataf_reg['flag_prod_top50'] = dataf_reg.isin(produtos).any(1).astype(int)

# Removendo as colunas desnecessárias no momento, como ID's e Datetime
cols_df_top50 = ['order_status','freight_value','payment_value','payment_sequential','payment_type','product_category_name','payment_installments','customer_city','customer_state','product_name_lenght','product_description_lenght','product_photos_qty','product_weight_g','product_length_cm','product_height_cm','product_width_cm','seller_city','seller_state','regiao_x','regiao_y','flag_prod_top50']
df_top50 = dataf_reg[cols_df_top50]

# Normalizando os dados (flag_remove para não normalizar o flag)
df_top50 = concatena_norm(df=df_top50, flag_remove='flag_prod_top50')

graf_import, df_top50_result = relevancia(df_top50, flag='flag_prod_top50')

"""O gráfico abaixo nos trás informações importantes.

O método utilizado permite uma maior confiança nos resultados em comparação com o método tradicional de impresa.

Fica visível abaixo que:

• As características físicas dos produtos possuem alta relevância enquanto as financeiras possuem as menores.

• Feactures informativas como a descrição e o nome do produto são mais importantes neste caso que a quantidade de fotos ou o preço.

• As informações menos relevantes são as que dizem respeito aos valores dos produtos e as formas de pagamento.
"""

graf_import

"""---

### Parcelamentos
1. [✓] Quais tipos de pagamento tem maior numero de cancelamento e maior média (e mediana) de reclamações?

**Quais tipos de pagamento tem maior numero de cancelamento?**
"""

dataf_reg['tempo_entrega_dias'] = dataf_reg['order_delivered_customer_date'].dt.date - dataf_reg['order_approved_at'].dt.date
dataf_reg['tempo_entrega_dias'] = dataf_reg['tempo_entrega_dias'].dt.days

pag_cancel = dataf_reg[(dataf_reg['order_status'] == 'canceled') | (dataf_reg['tempo_entrega_dias'] > 90)]['payment_type'].value_counts(normalize=True)
pag_cancel

"""**Quais tipos de pagamento tem maior média (e mediana) de reclamações?**"""

pag_med_recl = dataf_reg.groupby('payment_type').agg(['mean','median'])['review_score']
pag_med_recl

"""---

### Avaliações
1. [✓] Qual a distribuição das avaliações de vendas entregues?
2. [✓] A proporção de reclamações tem aumentado com o tempo?
3. [✓] Quais categorias de produtos com menores e maiores avaliações?
4. [✓] Qual a acurácia do site?
"""

rev = dataf_reg.loc[dataf_reg['order_status'] == 'delivered'].review_score.value_counts(normalize=True)
fig_avaliacoes_ent = px.bar(rev,text=round(rev*100,3), width=800, height=450)
fig_avaliacoes_ent.update_layout(title={'text':'Distribuição das Notas de Avaliação', 'x':0.5},
                  uniformtext_minsize=8, 
                  uniformtext_mode='hide', 
                  showlegend=False,
                  yaxis=dict(
                    title='% das Notas',
                    titlefont_size=16,
                    tickfont_size=14),
                  xaxis=dict(
                    title='Notas de Avaliação',
                    titlefont_size=16,
                    tickfont_size=14))

fig_avaliacoes_ent.update_traces(texttemplate='%{text:.3s}', textposition='outside')
fig_avaliacoes_ent.show()

"""---

**A proporção de reclamações tem aumentado com o tempo?**
"""

meses = []
for i in range(1,13):
  rev_baixo = dataf_reg.loc[(dataf_reg['order_approved_at'].dt.year == 2017) & (dataf_reg['order_approved_at'].dt.month == i)].query('review_score <= 3 ').shape[0]
  rev_med_alt = dataf_reg.loc[(dataf_reg['order_approved_at'].dt.year == 2017) & (dataf_reg['order_approved_at'].dt.month == i)].query('review_score > 3').shape[0]
  meses.append(round(rev_baixo/rev_med_alt,3))
meses = pd.Series(meses)
meses.index = np.arange(1,13)

fig_prop_recl = px.bar(meses,text=meses, width=800, height=450,)
fig_prop_recl.update_layout(title={'text':'Proporção de Reclamações ao Longo do Tempo', 'x':0.5},
                  uniformtext_minsize=8, 
                  uniformtext_mode='hide', 
                  showlegend=False,
                  yaxis=dict(
                    title='Proporção de Reclamações',
                    titlefont_size=16,
                    tickfont_size=14),
                  xaxis=dict(
                    title='Meses',
                    titlefont_size=16,
                    tickfont_size=14))

fig_prop_recl.update_traces(texttemplate='%{text:.3s}', textposition='outside')
fig_prop_recl.show()

"""---

**Quais categorias de produtos com menores e maiores avaliações?**
"""

categ = dataf_reg.groupby(['product_category_name','order_id']).agg('mean').reset_index()
categ = categ.groupby('product_category_name').agg('mean').reset_index()
categ_aval = pd.DataFrame(categ.sort_values('review_score',ascending=False)[['product_category_name','review_score']])
categ_aval

"""---

**Qual a acurácia do site?**
"""

# Qual a acurácia do estimador do site
query = """ 
with tb_macro as (
  select t1.order_id,
        strftime('%Y-%m-%d',t1.order_estimated_delivery_date) as dt_estimativa,
        strftime('%Y-%m-%d',t1.order_delivered_customer_date) as dt_entregue
  from dataf_reg as t1
  where t1.order_status = 'delivered'
),

tb_dif_dias as (
  select t1.*,
        julianday(t1.dt_entregue) - julianday(t1.dt_estimativa) as dif_dias
  from tb_macro as t1
  where t1.dt_entregue not null)

select t1.*,
        case when t1.dif_dias between -7 and 7 then 1
        else 0 end target 
from tb_dif_dias as t1
order by target , dif_dias desc
"""
acuracia_site = ps.sqldf(query)
print('ACURÁCIA:\n(0: % de Acerto; 1: % de Erro)\nMargem: ± 7 Dias')
acuracia_site.target.value_counts(normalize=True)

"""---

### Dashboard Interativo
"""

# Criando o subplot com 4 linhas e 2 colunas (8 plots)
fig = make_subplots(rows=4, cols=2,
                    # Definindo os tipos de cada gráfico
                    specs=[ [{'type': 'xy'}, {'type': 'xy'}],
                            [{'type': 'xy'}, {'type': 'xy'}],
                            [{'type': 'xy'}, {'type': 'domain'}],
                            [{'type': 'xy'}, {'type': 'domain'}] ],
                    # Definindo os títulos dos gráficos em ordem de apresentação
                    subplot_titles=(" Vendas por Período", " Vendas por Região", 
                                    " Notas de Avaliação", " Características Relevantes", 
                                    " Vendas X Frete", " Score das Categorias", 
                                    "Total de Vendas por Hora", "Região e o Percentual de Frete"))


# Título e Ocultação da Legenda
fig.update_layout(height=800, width=1400, title={"text":"Dashboard Exploratório: Rotatividade de Vendedores", "x":0.5})
fig.update_layout(showlegend=False)


# Plot 1  Vendas por Período
fig.add_trace(go.Scatter(
        x=vendas_period["data"],
        y=vendas_period["qtd_vendida"],
        name='Qtd Vendas',
        mode="lines"),row=1, col=1)

# Plot 2 | Vendas por Região
fig.add_trace(go.Bar(x= vendas_regiao['regiao_y'], y=vendas_regiao['payment_value'], name='Soma'),
              row=1, col=2)

# Plot 3 | Notas de Avaliação
fig.add_trace(go.Bar(x= rev.index, y=rev.apply(lambda x: round(x,3))*100, name='%'),
              row=2, col=1)

# Plot 4 | Características Relevantes
colsX = ['Altura','Peso','Descrição','Largura','Nome do Prod.','Comprimento','Fotos','Valor','Frete','Pag. Misto','Parcelamento']
df_top50_result.index = colsX
fig.add_trace(go.Bar(x= df_top50_result.index, y=df_top50_result.apply(lambda x: round(x,3))*100, name='%'),
              row=2, col=2)

# Plot 5 | Vendas X Frete
fig.add_trace(go.Scatter(x=dataf.payment_value[::10], y=dataf.freight_value, mode='markers',name='%', marker=dict(size=10, opacity=.5)),
              row=3, col=1)

# Plot 6 | Score das Categorias
categ_aval['review_score'] = categ_aval['review_score'].apply(lambda x: round(x,3))
fig.add_trace(go.Table(
        header=dict(
            values=["Categoria", "Nota Avaliação"],
            font=dict(size=10),
            align="left"
        ),
        cells=dict(
            values=[categ_aval[k].tolist() for k in categ_aval],
            align = "left")
    ),
              row=3, col=2)

# Plot 7 | Total de Vendas por Hora
fig.add_trace(go.Bar(x=vendas_horas['Horas'], y=vendas_horas['Qtd_vendas'], name='Qtd Vendas'),
              row=4, col=1)

reg_frete = reg_pay_feight.reset_index()
# Plot 8 | Região e o Percentual de Frete
reg_frete['%_frete'] = reg_frete['%_frete'].apply(lambda x: round(x,3))
fig.add_trace(go.Table(
        header=dict(
            values=["Região", "% de Frete"],
            font=dict(size=10),
            align="left"
        ),
        cells=dict(
            values=[reg_frete[k].tolist() for k in reg_frete[['regiao_x','%_frete']]],
            align = "left")
    ),
    row=4, col=2)

fig.show()